{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 : Mixture Models+Model orden selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the advanced Machine Learning Course.\n",
    "\n",
    "The objective of this lab session is to code a few regression algorithms and to apply them to synthetic and real datasets.\n",
    "\n",
    "Please put **\"ML - MDS - TD5\"** in the mail subject or I might lose your work (which means 0) and send it to pierre.houdouin@centralesupelec.fr\n",
    "\n",
    "Please label your notebook **\"L5_familyname1_familyname2.ipynb\"** or I might lose your work (which means 0).\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. After estimation of those parameters we get an estimation of the distribution of our data. For the clustering task, one can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians. \n",
    "\n",
    "### First part\n",
    "\n",
    "Fill in the following class to implement a multivariate GMM:\n",
    "\n",
    "$\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class my_GMM():\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        '''\n",
    "        Parameters:\n",
    "        k: integer\n",
    "            number of components\n",
    "        \n",
    "        Attributes:\n",
    "        \n",
    "        alpha_: np.array\n",
    "            proportion of components\n",
    "        mu_: np.array\n",
    "            array containing means\n",
    "        Sigma_: np.array\n",
    "            array cointaining covariance matrix\n",
    "        cond_prob_: (n, K) np.array\n",
    "            conditional probabilities for all data points \n",
    "        labels_: (n, ) np.array\n",
    "            labels for data points\n",
    "        '''\n",
    "        self.k = k\n",
    "        self.alpha_ = None\n",
    "        self.mu_ = None\n",
    "        self.Sigma_ = None\n",
    "        self.cond_prob_ = None\n",
    "        self.labels_ = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\" Find the parameters\n",
    "        that better fit the data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            Data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        self\n",
    "        \"\"\"\n",
    "        def compute_condition_prob_matrix(X, alpha, mu, Sigma):\n",
    "            '''Compute the conditional probability matrix \n",
    "            shape: (n, K)\n",
    "            '''\n",
    "        \n",
    "        # TODO:\n",
    "        # initialize the parameters\n",
    "        # apply sklearn kmeans or randomly initialize them\n",
    "        \n",
    "        # While not(convergence)\n",
    "        #     Compute conditional probability matrix\n",
    "        #     Update parameters\n",
    "        \n",
    "        # Update labels_\n",
    "        \n",
    "        # Return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict labels for X\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            New data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        label assigment        \n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        \n",
    "    def compute_proba(self, X):\n",
    "        \"\"\" Compute probability vector for X\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            New data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        proba: (n, k) np.array        \n",
    "        \"\"\"\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate your own mixture of Gaussian distributions to test the model, choose parameters so that GMM performs better than K-Means on it. Use `np.random.multivariate_normal`. \n",
    "\n",
    "Plot data with colors representing predicted labels and shapes representing real labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
