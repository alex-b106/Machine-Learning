{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the advanced Machine Learning Course.\n",
    "\n",
    "The objective of this lab session is to code a few regression algorithms and to apply them to synthetic and real datasets.\n",
    "\n",
    "Please put **\"ML - MDS - TD3\"** in the mail subject or I might lose your work (which means 0) and send it to pierre.houdouin@centralesupelec.fr\n",
    "\n",
    "Please label your notebook **\"L3_familyname1_familyname2.ipynb\"** or I might lose your work (which means 0).\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:22:29.735094800Z",
     "start_time": "2024-10-11T13:22:29.668500200Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the dataset that we are going to use, an indian dataset including in the last column information about the diabetes status of patients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:22:31.187162300Z",
     "start_time": "2024-10-11T13:22:29.746037500Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "data = pd.read_csv(\"w8a.csv\", sep=\";\", header=None)\n",
    "\n",
    "X = data.iloc[:,:-1].to_numpy()\n",
    "y = data.iloc[:,-1].to_numpy()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33, random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll be moving from linear regression to logistic regression, one of the simplest ways to deal with a classification problem. Instead of fitting a line, logistic regression models the probability that the outcome is 1 given the value of the predictor. In order to do this we need a function that transforms our predictor variable to a value between 0 and 1. Lots of functions can do that, but the logistic function is the most common choice:\n",
    "\n",
    "$$f(z) = \\frac{1}{1+\\exp{-z}}.$$\n",
    "\n",
    "To predict the class of our observations we'll have to minimize the corresponding loss function and as we are in a high-dimensional context we'll add an $l_2$ regularization to the model:\n",
    "\n",
    "$$L(\\textbf{w}) = \\sum_{i=1}^n log(1+\\exp(-y_i\\textbf{w}^Tx_i))+\\frac{\\lambda}{2} \\| \\textbf{w} \\|^2,$$\n",
    "\n",
    "where $x_i$ is the vector of features for the observation $i$ and $y_i \\in \\{-1, 1\\}$ is the class label.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use the `sklearn` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:22:43.963412100Z",
     "start_time": "2024-10-11T13:22:31.190303600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=0,solver='saga')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we compute the accuracy score to evaluate the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:22:43.986994500Z",
     "start_time": "2024-10-11T13:22:43.966609700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Implement from scratch your own logistic regression model with stochastic gradient descent optimization. \n",
    "\n",
    "- Fill in the class\n",
    "\n",
    "- Display the evolution of the cost function along iterations. Do this for several strategies for the setting of the learning rate\n",
    "\n",
    "- Try the different acceleration strategies\n",
    "\n",
    "- Train the model with the training set and evaluate its performance in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:22:44.016997900Z",
     "start_time": "2024-10-11T13:22:43.986994500Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 34 (3376351676.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[18], line 37\u001B[1;36m\u001B[0m\n\u001B[1;33m    def fit(self, X, y, lr=1e-2, bsize=64, max_iter=100, minibatch=False):\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m expected an indented block after function definition on line 34\n"
     ]
    }
   ],
   "source": [
    "class StochasticLogisticRegression():\n",
    "    \n",
    "    \"\"\" Class for logistic regression:\n",
    "    \n",
    "    Attributes:                                                                                 \n",
    "    -----------                                                                              Default value\n",
    "    coef_         : 1-dimensional np.array, coefficients / weights                         | None                \n",
    "    lambd_        : float,                  regularization parameter                       | 0.1\n",
    "    lr_           : float,                  the learning rate                              | 0.01\n",
    "    bsize         : integer,                the size of the mini-batch >=1                 | 64\n",
    "    gamma         : float,                  gamma coefficient                              | 0.999\n",
    "    beta          : float,                  beta coefficient                               | 0.9\n",
    "    eps           : float,                  epsilon coefficient                            | 10-8\n",
    "    debias        : boolean,                indicates if we use the debiais correction     | False\n",
    "    coef_history_ : list,                   the list of all visited betas/ weights         | []\n",
    "    f_history_    : list ,                  the list of all evaluations in visited betas   | []\n",
    "    thresh        : float,                  decision threshold for classification          | 0.5\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambd=0.1, lr=1e-2, batchsize=64, gamma=0.999, beta=0.9, eps=1e-8, debias=False,\n",
    "                 thresh=0.5):\n",
    "        self.coef_         = None # weights\n",
    "        self.lambd_        = lambd\n",
    "        self.lr_           = lr\n",
    "        self.bsize_        = batchsize\n",
    "        self.gamma_        = gamma\n",
    "        self.beta_         = beta\n",
    "        self.eps_          = eps\n",
    "        self.debias_       = debias\n",
    "        self.coef_history_ = []\n",
    "        self.f_history_    = []\n",
    "        self.thresh_       = thresh\n",
    "\n",
    "    def logistic(self, z):\n",
    "        #TODO\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y, lr=1e-2, bsize=64, max_iter=100, minibatch=False):\n",
    "        \n",
    "        \"\"\" Fit the data (X, y).\n",
    "    \n",
    "        Parameters:\n",
    "        -----------                                                                         Default value\n",
    "        X          : (num_samples, num_features) np.array, Design matrix                  | \n",
    "        y          : (num_sampes, ) np.array,              Output vector                  | \n",
    "        lr         : float,                                the learning rate              | 0.001\n",
    "        bsize      : integer,                              the size of the mini-batch >=1 | 64\n",
    "        max_iter   : integer,                              the number of epochs           | 100\n",
    "        mini_batch : bool,                                 method used                    | False\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Make binary predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array, Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array, Predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "    \n",
    "    def accuracy_evaluation(self,X,y,thresh=0.5,margin=0,vizualisation=True):\n",
    "        \"\"\" Compute detailed accuracy rates.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------                                                              Default value\n",
    "        X                : (num_samples, num_features) np.array, Input data    | \n",
    "        y                : (num_samples, ) np.array, Labels of the input data  | \n",
    "        thresh           : Decision threshold                                  | 0.5\n",
    "        margin           : If the prediction is at a distance less than margin | 0\n",
    "                           to the treshold, returns undetermined label.        |\n",
    "        vizualisation    : Allow the user to vizualize in a board the results  | True\n",
    "            \n",
    "        Returns:\n",
    "        -----\n",
    "        good_prediction  : % of correct classifications\n",
    "        undetermined     : % of indetermined labels \n",
    "        wrong_prediction : % of wrong classifications\n",
    "        TP               : % of 1 labelled 1\n",
    "        UP               : % of 1 labelled undetermined\n",
    "        FN               : % of 1 labelled 0\n",
    "        TN               : % of 0 labelled 0\n",
    "        UN               : % of 0 labelled undetermined\n",
    "        FP               : % of 0 labelled 1\n",
    "        F1_score         : F1-score\n",
    "        \"\"\"  \n",
    "        #TODO\n",
    "    \n",
    "    def find_thresh(self, X, y, step=0.01,margin=0):\n",
    "        \"\"\"Find the decision threshsold that maximize the f1_score\n",
    "        \n",
    "        Parameters:\n",
    "        -----------                                                    Default value\n",
    "        X      : (num_samples, num_features) np.array, Input data    | \n",
    "        y      : (num_samples, ) np.array, Labels of the input data  | \n",
    "        step   : Decision threshold                                  | 0.5\n",
    "        margin : If the prediction is at a distance less than margin | 0\n",
    "                 to the treshold, returns undetermined label.        |\n",
    "        \"\"\"  \n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie plusieurs entraînements avec différents coefficients de régularisation, on choisit celui qui minimise\n",
    "l'erreur sur le test set. On entraine sur 1000 itérations en batch gradient descent. On garde **lambda = 2**.\n",
    "On essaie ensuite plusieurs learning rates, on choisit le plus grand qui permet d'obtenir un gradient qui converge. On garde **lr = 0.01**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2024-10-11T13:22:44.012704600Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2024-10-11T13:22:44.014878500Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement only one acceleration method and compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of the batchsize impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:22:44.020206400Z",
     "start_time": "2024-10-11T13:22:44.018073300Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO : Study the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T13:22:44.035303500Z",
     "start_time": "2024-10-11T13:22:44.020206400Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO : Study the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-11T13:22:44.023483Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO : Study Loss and F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta impact\n",
    "Nous allons comparer les paramètres beta pour gamma = 0.999 et batch_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-11T13:22:44.025484600Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO : Study Loss and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-11T13:22:44.026611500Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
